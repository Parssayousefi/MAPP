---
title: "MAPP"
author: "Parsa Yousefi"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

if (!require("lavaan")) {
    install.packages("lavaan")
}
if (!require("pwr")) {
    install.packages("pwr")
}
if (!require("readxl")) {
    install.packages("readxl")
}
if (!require("ggplot2")) {
    install.packages("ggplot2")
}
if (!require("glmnet")) {
    install.packages("glmnet")
}

library(lavaan)
library(readxl)
library(ggplot2)
library(glmnet) # for lasso regression


```



Load Data
```{r}
# Load the data
mydata <- HolzingerSwineford1939
mydata$school <- NULL
head(mydata)

```

#Preprocessing
```{r}
#sampel size estimate

#testing assumptions


# Descriptive statistics, normality checks, etc.

# standardized residuals

#check each items if they are continious. 


#code for binary items in the qeustionaire.
#data$binary <- ifelse(data$item > 3, 0, 1)





```

#Calculate sum scores
```{r}
# Calculate sum scores for each scale
mydata$MEQ_sum <- rowSums(mydata[, paste0("MEQ", 1:30)], na.rm = TRUE)
mydata$CEQ_sum <- rowSums(mydata[, paste0("CEQ", 1:26)], na.rm = TRUE)
mydata$AWES_sum <- rowSums(mydata[, paste0("AWES", 1:30)], na.rm = TRUE)
mydata$ESAT_sum <- rowSums(mydata[, paste0("ESAT", 1:18)], na.rm = TRUE)
mydata$EDI_sum <- rowSums(mydata[, paste0("EDI", 1:8)], na.rm = TRUE)
mydata$SWLS_sum <- rowSums(mydata[, paste0("SWLS", 1:5)], na.rm = TRUE)
mydata$NADA_S_sum <- rowSums(mydata[, paste0("NADA_S", 1:3)], na.rm = TRUE)
mydata$ASC11D_sum <- rowSums(mydata[, paste0("ASC11D", 1:11)], na.rm = TRUE)
mydata$ASC11DShort_sum <- mydata[,"ASC11DShort1"]  # Only one item, so it's just the item itself
mydata$EISI_sum <- rowSums(mydata[, paste0("EISI", 1:25)], na.rm = TRUE)

# For scales ending with 'lastItem', you need to adjust the range according to the last item number
# Assuming 'lastItem' for INOE is 'n' and for APEI is 'm'
# Replace 'n' and 'm' with the actual last item numbers for INOE and APEI scales
mydata$INOE_sum <- rowSums(mydata[, paste0("INOE", 1:n)], na.rm = TRUE)
mydata$APEQ_S_sum <- rowSums(mydata[, paste0("APEQ_S", 1:12)], na.rm = TRUE)
mydata$LAP_sum <- rowSums(mydata[, paste0("LAP", 1:56)], na.rm = TRUE)
mydata$APEI_sum <- rowSums(mydata[, paste0("APEI", 1:m)], na.rm = TRUE)

# Note: Replace 'n' and 'm' with the actual numbers of the last items for INOE and APEI scales.
# Ensure the column names used in paste0() match exactly with those in your dataset.

```	

#Split Dataset
```{r}
# Split the data into training and testing sets
# Splitting the data across subjects
set.seed(123)  # Set random seed for reproducibility

# 1. Identify unique subjects 
unique_subjects <- unique(mydata$subjectID)  # Assuming you have a 'subjectID' column

# 2. Calculate number of subjects for training and testing sets
n_train <- round(0.70 * length(unique_subjects))  # 70% of subjects for training
n_test <- length(unique_subjects) - n_train

# 3. Randomly sample subjects for training and testing sets
train_subjects <- sample(unique_subjects, n_train)
test_subjects <- setdiff(unique_subjects, train_subjects)

# 4. Create training and testing datasets
train_data <- mydata[mydata$subjectID %in% train_subjects, ]
test_data  <- mydata[mydata$subjectID %in% test_subjects, ]

```

#Contingency Assumption
```{r}
#What if assumptions are not met?

#Assumption: manifest endogenous variables follow a multivariate normal distribution
# if violated: use "robust estimators"


```

#CFA: One factor mnodel per qeustionaire
```{r}
#One-Factor Models per Questionnaire: Begin with individual models for each questionnaire to facilitate debugging.
```


#CFA: One factor model for all items
```{r}
# assume data has one clumn for each item: MEQ1, MEQ2, ..., CEQ1, CEQ2, ..., AWE1, AWE2,
# and so on. The first column is the ID column, so we start at the second column (index 2)

# Constructing the model string
# Here we concatenate 'Mystic =~' with all item names, joined by ' + '

model_string <- paste("Mystic =~", paste(names(train_data)[2:length(names(train_data))], collapse=" + "))

# Print the model string to check it
# cat(modelString)

# Fit the model
fit_normal <- cfa(model_string, data = train_data, estimator = "ML")

# Summarize the fit
summary(fit_normal, fit.measures = TRUE, standardized = TRUE)
```
#check if model is correct based on grapohs
```{r}  
lavaanPlot(lavaan(HS.fit_normal))
```
#check residuals
```{r}

# standardized residuals
residuals <- lavResiduals(fit_normal)
str(residuals)


# Extract the standardized residuals matrix
residuals_matrix <- residuals$cov.z

# Convert the matrix to a data frame
residuals_df <- as.data.frame(as.table(residuals_matrix))

# Rename the columns to match your ggplot aes
names(residuals_df) <- c("Variable1", "Variable2", "Residual")

# heat map of residuals indicate the magnitude and direction of the residuals,
ggplot(residuals_df, aes(x = Variable1, y = Variable2, fill = Residual)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0) +
  theme_minimal() +
  labs(title = "Standardized Residuals",
       x = "Item 1",
       y = "Item 2",
       fill = "Residual")
 
# histogram of residuals
ggplot(residuals_df, aes(x = Residual)) +
    geom_histogram(bins = 30, fill = "blue", color = "black") +
    theme_minimal() +
    labs(title = "Histogram of Standardized Residuals", x = "Standardized Residual", y = "Frequency")

#Plot Q-Q Plot of Residuals
qqnorm(residuals_df$Residual, main = "Q-Q Plot of Standardized Residuals")
qqline(residuals_df$Residual, col = "red")
```


#Hierarchical Model
```{r}


# Higher-order model specification 
model_hierarchical <- '
  # Specific factors for each questionnaire
  MEQ =~ MEQ1 + MEQ2 + ... + MEQ30
  CEQ =~ CEQ1 + CEQ2 + ... + CEQ26
  AWES =~ AWES1 + AWES2 + ... + AWES30
  ESAT =~ ESAT1 + ESAT2 + ... + ESAT18
  EDI =~ EDI1 + EDI2 + ... + EDI8
  SWLS =~ SWLS1 + SWLS2 + ... + SWLS5
  NADA_S =~ NADA_S1 + NADA_S2 + NADA_S3
  ASC11D =~ ASC11D1 + ASC11D2 + ... + ASC11D11
  ASC11DShort =~ ASC11DShort1
  EISI =~ EISI1 + EISI2 + ... + EISI25
  INOE =~ INOE1 + INOE2 + ... + INOE_lastItem
  APEQ_S =~ APEQ_S1 + APEQ_S2 + ... + APEQ_S12
  LAP =~ LAP1 + LAP2 + ... + LAP56
  APEI =~ APEI1 + APEI2 + ... + APEI_lastItem

  # General factor "Mystic" influencing all specific factors
  Mystic =~ MEQ + CEQ + AWES + ESAT + EDI + SWLS + NADA_S + ASC11D + ASC11DShort + INOE + EISI + APEQ_S + LAP + APEI
'

# Fit the hierarchical model 
fit_hierarchical <- cfa(model_hierarchical, data = mydata, estimator = "ML")

# Summarize the model fit and interpret the results
summary(fit_hierarchical, fit.measures = TRUE, standardized = TRUE)


```
#check if model is correct based on grapohs
```{r}  
lavaanPlot(lavaan(HS.model_hierarchical))
```

#bi-factor Model
```{r}
# Bi-factor model specification
model_bifactor <- '
    # Specific factors for each questionnaire
  MEQ =~ MEQ1 + MEQ2 + ... + MEQ30
  CEQ =~ CEQ1 + CEQ2 + ... + CEQ26
  AWES =~ AWES1 + AWES2 + ... + AWES30
  ESAT =~ ESAT1 + ESAT2 + ... + ESAT18
  EDI =~ EDI1 + EDI2 + ... + EDI8
  SWLS =~ SWLS1 + SWLS2 + ... + SWLS5
  NADA_S =~ NADA_S1 + NADA_S2 + NADA_S3
  ASC11D =~ ASC11D1 + ASC11D2 + ... + ASC11D11
  ASC11DShort =~ ASC11DShort1
  EISI =~ EISI1 + EISI2 + ... + EISI25
  INOE =~ INOE1 + INOE2 + ... + INOE_lastItem
  APEQ_S =~ APEQ_S1 + APEQ_S2 + ... + APEQ_S12
  LAP =~ LAP1 + LAP2 + ... + LAP56
  APEI =~ APEI1 + APEI2 + ... + APEI_lastItem

 # General factor "mystic" affecting all items
  Mystic =~ MEQ1 + MEQ2 + ... + APEI_lastItem
  paste("Mystic =~", paste(names(data)[2:251], collapse=" + "))

'

# Fit the bi-factor model
# orthogonal = TRUE makes all exogenous LVs in the model uncorrelated
#because we constrained the loadings for the domain-specific LVs with only two indicator 
#If we do not use this argument, then all the constrained factor loadings are set to 1 
fit_bifactor <- cfa(model_bifactor, data = mydata, orthogonal = TRUE, std.lv=TRUE, estimator = "ML")

# Summarize the model fit and interpret the results
summary(fit_bifactor, fit.measures = TRUE, standardized = TRUE)
```	
#check if model is correct based on grapohs
```{r}  
lavaanPlot(lavaan(HS.model_bifactor))
```

#lav test: ist die abweichung der smnqaple corr matrix gfroß  genug von der model based matrix um die hypo zu verwerfen, dass das model wahr ist. 
# p wert für 0 hypothesis test. 
```{r}

#lavtest(lavaan object)

```

```{r}'

# Compare the models
```{r}
# Models are nested! 
#From Most restrictive to least restrictive: CFA, Hierarchical, Bi-factor
anova( fit_normal, fit_hierarchical )
anova(fit_hierarchical, fit_bifactor)

```

#sum scores
```{r}

# fit sum scores 


```
#Sensitivity Analyses
```{r}
# Sensitivity analyses

```

#Contingency Fit
```{r}
#What if the initial model doesnt fit? Modi indices!


```

# What is the best predictor of well being?
```{r}	
# Step 1: Perform Lasso Regression
# Assuming 'wellbeing' is the outcome variable and scale sum scores are predictors
# Prepare predictors and outcome variables using scale sum scores
predictors_scales <- train_data[, c("MEQ_sum", "CEQ_sum", "AWES_sum", "ESAT_sum", "EDI_sum", "SWLS_sum", "NADA_S_sum", "ASC11D_sum", "ASC11DShort_sum", "EISI_sum", "INOE_sum", "APEQ_S_sum", "LAP_sum", "APEI_sum")]
outcome <- train_data$wellbeing

# Fit Lasso model using scale scores
set.seed(123) # for reproducibility
lasso_model_scales <- cv.glmnet(as.matrix(predictors_scales), outcome, alpha = 1, family = "gaussian")

# Extract and print selected variables
best_lambda_scales <- lasso_model_scales$lambda.min
selected_vars_scales <- coef(lasso_model_scales, s = best_lambda_scales)[,-1] # excluding intercept
print(selected_vars_scales)

# Fit Lasso model using direct item analysis
# Replace 'item1', 'item2', ... with column names
predictors_items <- train_data[, c("item1", "item2", "item3", ...)] 
outcome <- train_data$wellbeing
lasso_model_items <- cv.glmnet(as.matrix(predictors_items), outcome, alpha = 1, family = "gaussian")

# Extract and print selected variables
best_lambda_items <- lasso_model_items$lambda.min
selected_vars_items <- coef(lasso_model_items, s = best_lambda_items)[,-1] # excluding intercept
print(selected_vars_items)


# Step 2: Correlational Analysis
# Correlation of scale scores with well-being
correlations_scales <- cor(predictors_scales, outcome)
print(correlations_scales)


# Correlation of Items  with well-being
# Assuming 'items' dataframe contains all individual items
# Replace 'predictors_items' with the dataframe containing all your items
correlations_items <- cor(predictors_items, outcome)
print(correlations_items)


# Step 3: Fit a 1-Factor Model for Well-being Questionnaires # might not be needed!
# Assuming 'WB1' to 'WBn' are your well-being questionnaire items
wb_model_string <- "WB =~ WB1 + WB2 + ... + WBn" # Adjust this line
wb_fit <- cfa(wb_model_string, data = train_data, estimator = "ML")
summary(wb_fit, fit.measures = TRUE, standardized = TRUE)

# Extract factor scores if the model fits well
factor_scores <- lavPredict(wb_fit)
train_data$WB_factor_score <- factor_scores[, 1]

```
